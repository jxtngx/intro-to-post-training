{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1I-U4MzijBA7V8Z8Tpq2WgI0MMJuq7pN8",
      "authorship_tag": "ABX9TyNjve/HukzRRelE5PMrH81O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jxtngx/torchtune-cookbook/blob/main/summarization/L1_Summarization_with_Llama3_2_1B_and_torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Make certain to switch to a T4 GPU Runtime!"
      ],
      "metadata": {
        "id": "xNqTstvuO2KD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's follow the installation instructions given in the torchtune README"
      ],
      "metadata": {
        "id": "94RiLM50mUa6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sggK5iIZXA4j"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install torch torchvision torchao -q\n",
        "pip install git+https://github.com/pytorch/torchtune.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that installation is complete, let's download the Llama 3.2 1B Instruct Model\n",
        "\n",
        "> make certain to set the output dir to <br/>\n",
        "> ${PWD}/Meta-Llama-3.1-8B-Instruct"
      ],
      "metadata": {
        "id": "WbdpMvsZPlS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "tune download meta-llama/Llama-3.2-1B-Instruct \\\n",
        "--output-dir ${PWD}/Llama-3.2-1B-Instruct \\\n",
        "--ignore-patterns \"original/consolidated.00.pth\" \\\n",
        "--hf-token <<YOUR_HF_KEY>>"
      ],
      "metadata": {
        "id": "WS5L_3oEPbnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's load the data from the Lesson 0 notebook on data acquisition and preprocessing."
      ],
      "metadata": {
        "id": "cx6npmuoS80-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "eASug8ACTAVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/intelligent-agents/intelligent_agent.json\", \"r\") as fp:\n",
        "    data = json.load(fp)"
      ],
      "metadata": {
        "id": "wOJceHQlTB5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the key is \"Intelligent Agent\"\n",
        "data.keys()"
      ],
      "metadata": {
        "id": "A_84_wjITRR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remind ourselves that the article key should have as an item, another dict\n",
        "# with \"url\" and \"body\" keys\n",
        "data[\"Intelligent Agent\"].keys()"
      ],
      "metadata": {
        "id": "rb6Gcvm3TSz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the text data we are after is in the body key\n",
        "# let's save that to a variable named msg\n",
        "# and be certain to add the instructions to summarize the article\n",
        "# before using torchtune.generate\n",
        "msg = \"please summarize the following article: \"+ data[\"Intelligent Agent\"][\"body\"]"
      ],
      "metadata": {
        "id": "7krrB_0NTW77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to run generation. We can use torchtune for this.\n",
        "\n",
        "> `torchtune.generation.generate` returns token IDs and logits. the method is not designed as a chat interface."
      ],
      "metadata": {
        "id": "7G8CpXOlSxQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "import torchtune\n",
        "import torchao\n",
        "from torchao.quantization.quant_api import int8_weight_only\n",
        "from torchtune.generation import generate\n",
        "from torchtune.training.checkpointing import FullModelHFCheckpointer\n",
        "from torchtune.models.llama3_2 import llama3_2_1b\n",
        "from torchtune.models.llama3 import llama3_tokenizer"
      ],
      "metadata": {
        "id": "JENgiVCJS2zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load the checkpoint we downloaded from Hugging Face."
      ],
      "metadata": {
        "id": "8vFBYnVdn7va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a checkpointer\n",
        "ckptr = FullModelHFCheckpointer(\n",
        "    \"/content/Llama-3.2-1B-Instruct/\",\n",
        "    checkpoint_files = [\"model.safetensors\"],\n",
        "    model_type=\"LLAMA3_2\",\n",
        "    output_dir=\"/content/output/Llama-3.2-1B-Instruct\"\n",
        "    )"
      ],
      "metadata": {
        "id": "xdg5J1YmUC8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the checkpoint\n",
        "# note: this returns a model state dict that needs to be loaded to a model in the following cell\n",
        "model_sd = ckptr.load_checkpoint()"
      ],
      "metadata": {
        "id": "HUxDgT9enmlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a model and load the state_dict\n",
        "model = llama3_2_1b()\n",
        "model.load_state_dict(model_sd[\"model\"])"
      ],
      "metadata": {
        "id": "PQynqJ5DvKyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qauntize the model with torchao\n",
        "torchao.quantize_(model, int8_weight_only(group_size=32))"
      ],
      "metadata": {
        "id": "imbWzE9X00Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "id": "HKMi3bh6n5Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's load the tokenizer.\n",
        "\n",
        "> Llama 3 models in torchtune reuse the `llama3_tokenizer`"
      ],
      "metadata": {
        "id": "9FbkhGbGohqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = llama3_tokenizer(\"/content/Llama-3.2-1B-Instruct/original/tokenizer.model\")"
      ],
      "metadata": {
        "id": "08BjMYarWl8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a basic prompt for a first pass at generation:"
      ],
      "metadata": {
        "id": "I9nZCph7o56C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = tokenizer.encode(\"Hi my name is\")"
      ],
      "metadata": {
        "id": "PBeHxyAZo4B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = torch.Generator(device=\"cuda\")\n",
        "rng.manual_seed(42)\n",
        "\n",
        "start = perf_counter()\n",
        "output, logits = generate(\n",
        "    model,\n",
        "    torch.tensor(prompt),\n",
        "    max_generated_tokens=100,\n",
        "    pad_id=0,\n",
        "    rng=rng\n",
        ")\n",
        "end = perf_counter()"
      ],
      "metadata": {
        "id": "W16IIL7XdSO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f\"generation took {(end-start)/60} minutes\""
      ],
      "metadata": {
        "id": "RO0qdaaxBGEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the output token IDs:"
      ],
      "metadata": {
        "id": "6PAhxTtQpBYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "07Z63YEQ74gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, let's decode the token IDs with the tokenizer:"
      ],
      "metadata": {
        "id": "x1BFoSSmpH4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output[0].tolist(), truncate_at_eos=False)"
      ],
      "metadata": {
        "id": "3YJe4pOHAj9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO Create summarization example"
      ],
      "metadata": {
        "id": "l07nP90hV3-K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Ha9weoyV-vT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}